# Sample Questions for Demo

**Purpose:** Pre-tested questions that reliably demonstrate Lumiere's capabilities  
**Status:** âœ… Verified on deployment  
**Last Updated:** December 23, 2025

---

## ğŸ“‹ Document Preparation

### Recommended Documents to Upload:

1. **AI/ML Technical Document** (Primary)
   - Example: "Introduction to Machine Learning" PDF
   - Why: Contains clear concepts for RAG demonstration
   - Size: 5-20 pages ideal
   - Download: Use any ML tutorial or textbook chapter

2. **Business Report** (Secondary)
   - Example: Company annual report, market analysis
   - Why: Shows enterprise use case
   - Size: 10-30 pages
   - Optional for time

3. **Research Paper** (Tertiary)
   - Example: Academic paper on NLP/LLMs
   - Why: Demonstrates technical content handling
   - Size: 8-12 pages
   - Optional for time

### Quick Test Documents:
If you need a quick document, create a simple text file:

```text
Title: Machine Learning Basics

Machine learning has three main types:

1. Supervised Learning
Supervised learning uses labeled training data. The algorithm learns 
from examples where the correct answer is known. Common algorithms 
include linear regression, decision trees, and neural networks.

2. Unsupervised Learning  
Unsupervised learning works with unlabeled data. The algorithm finds 
patterns without being told what to look for. Examples include 
clustering algorithms like K-means and dimensionality reduction 
techniques like PCA.

3. Reinforcement Learning
Reinforcement learning learns through trial and error. An agent takes 
actions in an environment and receives rewards or penalties. This is 
used in robotics, game playing, and autonomous systems.

Deep learning is a subset of machine learning that uses neural networks 
with multiple layers. It has revolutionized computer vision, natural 
language processing, and speech recognition.
```

Save as: `ml_basics.txt` or convert to PDF

---

## ğŸ¯ Mode 1: All-In Mode Questions

### Question Set A: RAG-Only Query

**Question 1 (Simple):**
```
What are the main types of machine learning mentioned in the document?
```

**Expected Behavior:**
- âœ… Routes to: Intent â†’ Retrieval â†’ Reasoning â†’ Critic â†’ Memory
- âœ… Shows: Answer with bullet points (supervised, unsupervised, reinforcement)
- âœ… Sources: Document chunks cited at bottom
- âœ… Time: ~5-8 seconds

**Question 2 (Definition):**
```
Explain supervised learning based on the document
```

**Expected Behavior:**
- âœ… Routes to: Same path as Q1
- âœ… Shows: Definition with examples from document
- âœ… Sources: Specific chunk containing supervised learning section
- âœ… Time: ~5-8 seconds

---

### Question Set B: SQL-Only Query

**Question 1 (Simple Select):**
```
Show me all customers
```

**Expected Behavior:**
- âœ… Routes to: Intent â†’ SQL Execution â†’ SQL Reasoning â†’ Critic â†’ Memory
- âœ… Shows: Table with customer data
- âœ… SQL Query: `SELECT * FROM customers LIMIT 10;`
- âœ… Time: ~3-5 seconds

**Question 2 (Aggregation):**
```
What is the total sales amount?
```

**Expected Behavior:**
- âœ… Routes to: Same SQL path
- âœ… Shows: Single number (sum of sales)
- âœ… SQL Query: `SELECT SUM(amount) FROM sales;`
- âœ… Time: ~3-5 seconds

**Question 3 (Top N):**
```
Show me the top 5 customers by total sales
```

**Expected Behavior:**
- âœ… Routes to: Same SQL path
- âœ… Shows: Table with 5 rows (customer name, total)
- âœ… SQL Query: JOIN with GROUP BY and ORDER BY
- âœ… Time: ~4-6 seconds

---

### Question Set C: Visualization Queries

**Question 1 (Bar Chart):**
```
Create a bar chart showing total sales by car model
```

**Expected Behavior:**
- âœ… Routes to: Intent â†’ SQL Execution â†’ SQL Reasoning â†’ Visualization â†’ Critic â†’ Memory
- âœ… Shows: Interactive Plotly bar chart
- âœ… SQL Query: JOIN with GROUP BY
- âœ… Chart: X-axis=car models, Y-axis=sales amount
- âœ… Time: ~6-10 seconds

**Question 2 (Comparison):**
```
Visualize sales comparison between different car brands
```

**Expected Behavior:**
- âœ… Routes to: Same path
- âœ… Shows: Bar or pie chart
- âœ… Interactive: Hover shows exact values
- âœ… Time: ~6-10 seconds

**Question 3 (Time Series - if date data available):**
```
Show me the sales trend over time
```

**Expected Behavior:**
- âœ… Routes to: Same path
- âœ… Shows: Line chart
- âœ… X-axis: Time, Y-axis: Sales
- âœ… Time: ~6-10 seconds

---

### Question Set D: Hybrid Queries (â­ SHOWCASE)

**Question 1 (RAG + SQL):**
```
Based on the document, explain supervised learning and then show me the sales data for customers who made purchases over $1000
```

**Expected Behavior:**
- âœ… Routes to: Intent â†’ Retrieval â†’ Reasoning â†’ SQL Execution â†’ SQL Reasoning â†’ Critic â†’ Memory
- âœ… Shows: 
  1. Explanation of supervised learning from document
  2. Table of customers with sales > $1000
- âœ… Demonstrates: Multi-step reasoning
- âœ… Time: ~10-15 seconds

**Question 2 (General + SQL):**
```
What is the difference between classification and regression, and show me which customers bought SUVs
```

**Expected Behavior:**
- âœ… Routes to: Intent â†’ General Reasoning â†’ SQL path
- âœ… Shows: 
  1. Explanation of classification vs regression
  2. Table of customers who bought SUVs
- âœ… Time: ~10-15 seconds

---

### Question Set E: General Knowledge Queries

**Question 1 (No RAG Needed):**
```
What is Python?
```

**Expected Behavior:**
- âœ… Routes to: Intent â†’ General Reasoning â†’ Critic â†’ Memory
- âœ… Shows: General explanation of Python (not from documents)
- âœ… Note: Shows system can handle queries outside uploaded docs
- âœ… Time: ~4-6 seconds

**Question 2 (Current Events):**
```
Explain what GPT-4 is
```

**Expected Behavior:**
- âœ… Routes to: Same general path
- âœ… Shows: General knowledge answer
- âœ… Time: ~4-6 seconds

---

## ğŸ¯ Mode 2: Chat with RAG Questions

### Question Set F: Document Q&A

**Question 1 (Direct):**
```
What types of machine learning are discussed?
```

**Expected Behavior:**
- âœ… Routes to: Intent â†’ Retrieval â†’ Reasoning â†’ Critic â†’ Memory
- âœ… Shows: Answer from document only
- âœ… Time: ~5-8 seconds

**Question 2 (Follow-up - CONTEXT DEMO):**
```
Can you explain that in simpler terms?
```

**Expected Behavior:**
- âœ… Uses conversation history to understand "that" = previous topic
- âœ… Shows: Simplified explanation of same content
- âœ… Demonstrates: Context awareness
- âœ… Time: ~5-8 seconds

**Question 3 (Anaphora Resolution):**
```
Give me an example of it
```

**Expected Behavior:**
- âœ… Resolves "it" from conversation history
- âœ… Shows: Example from document
- âœ… Time: ~5-8 seconds

---

### Question Set G: Source Verification

**Question 1 (Specific):**
```
What page discusses neural networks?
```

**Expected Behavior:**
- âœ… Shows: Answer with specific source citation
- âœ… Sources: [doc_id:chunk_index] clearly visible
- âœ… Demonstrates: Traceability
- âœ… Time: ~5-8 seconds

**Question 2 (Multi-source):**
```
Compare the definitions of supervised and unsupervised learning
```

**Expected Behavior:**
- âœ… Shows: Comparison from document
- âœ… Sources: Multiple chunks cited
- âœ… Time: ~6-10 seconds

---

## ğŸ¯ Mode 3: Data Analyst Questions

### Question Set H: Basic SQL

**Question 1 (Simple):**
```
How many customers do we have?
```

**Expected Behavior:**
- âœ… SQL: `SELECT COUNT(*) FROM customers;`
- âœ… Shows: Single number
- âœ… Time: ~3-5 seconds

**Question 2 (Filter):**
```
Show me customers from New York
```

**Expected Behavior:**
- âœ… SQL: `SELECT * FROM customers WHERE city = 'New York';`
- âœ… Shows: Filtered table
- âœ… Time: ~3-5 seconds

---

### Question Set I: Complex SQL

**Question 1 (JOIN):**
```
Which customers bought the most expensive cars?
```

**Expected Behavior:**
- âœ… SQL: JOIN customers, sales, cars with ORDER BY price
- âœ… Shows: Table with customer names and car details
- âœ… Time: ~5-8 seconds

**Question 2 (Aggregation + GROUP BY):**
```
What is the average sale amount per customer?
```

**Expected Behavior:**
- âœ… SQL: GROUP BY with AVG()
- âœ… Shows: Customer name and average
- âœ… Time: ~5-8 seconds

---

### Question Set J: Visualizations

**Question 1 (Distribution):**
```
Create a histogram of car prices
```

**Expected Behavior:**
- âœ… Shows: Histogram (Plotly)
- âœ… Interactive: Hover shows bin counts
- âœ… Time: ~6-10 seconds

**Question 2 (Comparison):**
```
Show a pie chart of sales by car type
```

**Expected Behavior:**
- âœ… Shows: Pie chart
- âœ… Interactive: Click to highlight segments
- âœ… Time: ~6-10 seconds

**Question 3 (Multi-series):**
```
Compare sales performance across different car brands
```

**Expected Behavior:**
- âœ… Shows: Grouped bar chart or stacked chart
- âœ… Time: ~6-10 seconds

---

## ğŸ”„ Context & Memory Demonstration

### Question Sequence K: Memory Test

**Step 1:**
```
What is supervised learning?
```
â†’ Get answer from document

**Step 2:**
```
Can you simplify that?
```
â†’ System remembers "that" = supervised learning

**Step 3:**
```
Give me a real-world example
```
â†’ System continues the conversation thread

**Step 4:**
```
How does this differ from unsupervised learning?
```
â†’ System uses "this" from context

**Expected Behavior:**
- âœ… All 4 questions answered correctly without re-specifying topic
- âœ… Demonstrates: Natural conversation flow
- âœ… Shows: Session memory working

---

## ğŸš¨ Edge Cases & Error Handling

### Question Set L: Error Scenarios

**Question 1 (No Documents):**
```
Tell me about quantum computing from my documents
```

**Expected Behavior (if no relevant docs uploaded):**
- âœ… System: "I don't have documents about quantum computing"
- âœ… Fallback: May route to general reasoning
- âœ… Shows: Graceful error handling

**Question 2 (Ambiguous SQL):**
```
Show me the data
```

**Expected Behavior:**
- âœ… System: Asks for clarification OR shows all tables
- âœ… Shows: Handles vague queries

**Question 3 (Invalid SQL Request):**
```
Delete all customers
```

**Expected Behavior:**
- âœ… System: Only SELECT queries allowed (read-only)
- âœ… Shows: Security measures

---

## âœ… Pre-Demo Testing Checklist

Test these the morning of your defense:

- [ ] Upload document successfully
- [ ] Test Q1 from Set A (simple RAG)
- [ ] Test Q1 from Set C (visualization)
- [ ] Test Q1 from Set D (hybrid query)
- [ ] Test follow-up question (Set F Q2)
- [ ] Check LangSmith traces are logging
- [ ] Verify source citations appear
- [ ] Confirm charts are interactive

---

## ğŸ“Š Question Selection Strategy

### For 12-Minute Demo:

**Must Include (Core Features):**
1. âœ… Document upload (Set A)
2. âœ… Simple RAG query (Set A Q1)
3. âœ… Hybrid query (Set D Q1) â­ SHOWCASE
4. âœ… Follow-up with context (Set F Q2) â­ MEMORY DEMO
5. âœ… SQL query (Set B Q3)
6. âœ… Visualization (Set C Q1) â­ VISUAL IMPACT

**Optional (Time Permitting):**
7. General knowledge query (Set E)
8. Error handling (Set L)
9. Complex SQL (Set I)

**Skip If Short on Time:**
- Multiple visualizations
- All three modes (focus on All-In)
- Edge cases

---

## ğŸ’¡ Tips for Question Selection

1. **Pre-test Everything**
   - Run each question you plan to use 2-3 times
   - Note the exact response time
   - Verify outputs are good quality

2. **Have Backups**
   - Prepare 2-3 backup questions per category
   - If one fails, switch to backup immediately

3. **Know Your Answers**
   - Memorize what the system SHOULD return
   - Spot errors immediately during demo

4. **Time Management**
   - Practice with a timer
   - Know which questions to skip if running late

---

## ğŸ¯ Golden Rule

**"If it doesn't work in practice, don't use it in the demo!"**

Only include questions you've successfully tested multiple times. Murphy's Law applies to live demos.

---

**Ready to impress! ğŸš€**
