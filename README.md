# ğŸŒŸ Lumiere â€” Agentic RAG Knowledge Workspace

> An intelligent multi-agent system combining RAG, SQL data analysis, and semantic memory for context-aware interactions

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.28+-FF4B4B.svg)](https://streamlit.io/)
[![Qdrant](https://img.shields.io/badge/Qdrant-Vector_DB-6C5CE7.svg)](https://qdrant.tech/)
[![LangChain](https://img.shields.io/badge/LangChain-ğŸ¦œ-00A67E.svg)](https://www.langchain.com/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

![Lumiere Architecture](docs/graph_visualization.png)

---

## ğŸ¯ Project Vision

**Lumiere is an open-source, agentic RAG knowledge workspace that uses multi-agent reasoning, long- and short-term memory, and Qdrant-backed retrieval, with full observability via Langfuse.**

Lumiere transforms traditional Q&A systems into an **intelligent assistant that learns and adapts** through semantic memory, supporting multiple interaction modes:
- ğŸ“š **RAG Mode**: Document-grounded responses with semantic search
- ğŸ“Š **Data Analyst Mode**: SQL queries with automated visualizations
- ğŸ’¬ **General Chat**: Conversational AI with context awareness
- ğŸ§  **Semantic Memory**: Long-term learning from past interactions

---

## âœ¨ Key Features

### ğŸ¤– Multi-Agent Architecture
- **Intent Agent**: Classifies queries and retrieves relevant memories
- **Reasoning Agent**: Generates grounded answers from retrieved context
- **SQL Agent**: Executes database queries and interprets results
- **Critic Agent**: Validates answer quality before storage
- **Visualization Agent**: Creates data visualizations (Data Analyst mode)

### ğŸ§  Semantic Memory System
- **Long-term memory** stored in Qdrant vector database
- **Automatic learning** from successful interactions
- **Context-aware responses** using past conversations
- **Quality filtering** via critic agent
- **Cross-session continuity** for personalized experiences

### ğŸ“Š Data Analysis & Visualization
- **Natural language to SQL** query generation
- **Automated chart creation** (bar, line, pie, scatter)
- **Interactive visualizations** with Plotly
- **Multi-table support** with SQLite backend

### ğŸ” Advanced RAG
- **Hybrid chunking** with semantic overlap
- **Vector similarity search** using OpenAI embeddings
- **Metadata filtering** for precise retrieval
- **Source attribution** for transparency
- **Pronoun resolution** for conversational context

### ğŸ“ˆ Observability
- **Langfuse integration** for trace analysis
- **Token usage tracking** per operation
- **Performance metrics** for all agents
- **Debug logging** with emoji indicators
- **Memory statistics** dashboard

---

## ğŸ—ï¸ Architecture

### System Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User      â”‚
â”‚  (Streamlit)â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         LangGraph Workflow              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Intent â†’ Retrieve/SQL/General   â”‚  â”‚
â”‚  â”‚     â†“           â†“                 â”‚  â”‚
â”‚  â”‚  Reason  â†’   Critic  â†’  Memory   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Qdrant    â”‚   â”‚   SQLite     â”‚
    â”‚  (Vectors)  â”‚   â”‚  (Data)      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Workflow Paths

1. **RAG Query Path**
   ```
   Intent (needs_rag) â†’ Retrieve â†’ Reason â†’ Critic â†’ Memory â†’ END
   ```

2. **SQL/Data Analysis Path**
   ```
   Intent (needs_sql) â†’ SQL Execute â†’ SQL Reason â†’ [Visualize] â†’ Critic â†’ Memory â†’ END
   ```

3. **General Chat Path**
   ```
   Intent (general) â†’ General Reason â†’ Critic â†’ Memory â†’ END
   ```

See [GRAPH_ARCHITECTURE.md](docs/GRAPH_ARCHITECTURE.md) for detailed workflow documentation.

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11+
- Qdrant (running locally or cloud)
- OpenAI API key
- Langfuse account (optional, for observability)

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/kikomatchi/lumiere.git
   cd lumiere
   ```

2. **Create virtual environment**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Set up environment variables**
   
   Create a `.env` file in the project root:
   ```env
   # OpenAI API
   OPENAI_API_KEY=your_openai_api_key_here
   
   # Qdrant Configuration
   QDRANT_URL=http://localhost:6333
   QDRANT_API_KEY=  # Optional, for Qdrant Cloud
   
   # Langfuse (Optional - for observability)
   LANGFUSE_PUBLIC_KEY=your_public_key
   LANGFUSE_SECRET_KEY=your_secret_key
   LANGFUSE_HOST=https://cloud.langfuse.com
   ```

5. **Start Qdrant** (if running locally)
   ```bash
   docker run -p 6333:6333 -p 6334:6334 \
       -v $(pwd)/qdrant_storage:/qdrant/storage:z \
       qdrant/qdrant
   ```

6. **Initialize Qdrant collections**
   ```bash
   python -c "from rag.collections import init_all_collections; init_all_collections()"
   ```

7. **Initialize semantic memory** (optional - seeds initial memories)
   ```bash
   python scripts/init_semantic_memory.py
   ```

8. **Run the application**
   ```bash
   streamlit run app.py
   ```

9. **Open your browser**
   
   Navigate to `http://localhost:8501`

---

## ğŸ“– Usage Guide

### 1. Ingesting Documents

**Via Streamlit UI:**
1. Click "ğŸ“„ Document Ingestion" in sidebar
2. Upload PDF, TXT, or MD files
3. Click "Ingest Documents"
4. Wait for confirmation

**Via Script:**
```bash
python -c "from rag.ingest import ingest_directory; ingest_directory('path/to/docs')"
```

### 2. Asking Questions

#### RAG Queries (Document-based)
```
"What is FFXIV?"
"Explain vector databases"
"How does semantic search work?"
```

#### Data Analysis Queries
```
"Show me the top 5 products by sales"
"How many hybrid cars are in the database?"
"What is the average price by manufacturer?"
```

#### General Chat
```
"Hello, how are you?"
"Can you help me with my project?"
"What can you do?"
```

### 3. Viewing Semantic Memory

**In Streamlit:**
1. Expand "ğŸ§  Semantic Memory" in sidebar
2. View total memories and types
3. Search memories by keyword
4. See relevance scores and timestamps

**Via Python:**
```python
from memory.semantic_memory import get_memory_stats, retrieve_memories

# Get statistics
stats = get_memory_stats()
print(stats)

# Search memories
memories = retrieve_memories(
    query="database queries",
    top_k=5,
    user_id="user123",
    min_score=0.7
)
```

### 4. Switching Modes

Use the sidebar to select:
- **All In**: All features enabled (default)
- **Chat + RAG**: Document Q&A only
- **Data Analyst**: SQL queries + visualizations

---

## ğŸ—‚ï¸ Project Structure

```
Lumiere/
â”œâ”€â”€ agents/                    # Agent implementations
â”‚   â”œâ”€â”€ intent_agent.py       # Intent classification + memory retrieval
â”‚   â”œâ”€â”€ reasoning_agent.py    # RAG reasoning
â”‚   â”œâ”€â”€ sql_agent.py          # SQL generation & execution
â”‚   â”œâ”€â”€ critic_agent.py       # Quality validation
â”‚   â””â”€â”€ viz_agent.py          # Visualization generation
â”‚
â”œâ”€â”€ graph/                     # LangGraph workflow
â”‚   â”œâ”€â”€ rag_graph.py          # Main graph definition
â”‚   â””â”€â”€ state.py              # State management
â”‚
â”œâ”€â”€ memory/                    # Semantic memory system
â”‚   â””â”€â”€ semantic_memory.py    # Vector-based memory storage/retrieval
â”‚
â”œâ”€â”€ rag/                       # RAG components
â”‚   â”œâ”€â”€ chunking.py           # Document chunking strategies
â”‚   â”œâ”€â”€ collections.py        # Qdrant collection management
â”‚   â”œâ”€â”€ embeddings.py         # OpenAI embeddings wrapper
â”‚   â”œâ”€â”€ ingest.py             # Document ingestion pipeline
â”‚   â”œâ”€â”€ qdrant_client.py      # Qdrant client singleton
â”‚   â””â”€â”€ retriever.py          # Semantic search & filtering
â”‚
â”œâ”€â”€ database/                  # Data storage
â”‚   â””â”€â”€ sqlite_client.py      # SQLite connection & queries
â”‚
â”œâ”€â”€ config/                    # Configuration
â”‚   â””â”€â”€ settings.py           # Environment & settings
â”‚
â”œâ”€â”€ scripts/                   # Utility scripts
â”‚   â”œâ”€â”€ init_semantic_memory.py   # Initialize memory system
â”‚   â”œâ”€â”€ ingest_test.py            # Test document ingestion
â”‚   â””â”€â”€ retrieval_test.py         # Test retrieval
â”‚
â”œâ”€â”€ ui/                        # Streamlit components
â”‚   â””â”€â”€ (UI modules)
â”‚
â”œâ”€â”€ app.py                     # Main Streamlit application
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ graph_visualization.mmd    # Mermaid diagram
â”œâ”€â”€ graph_visualization.png    # Architecture diagram
â”œâ”€â”€ GRAPH_ARCHITECTURE.md      # Detailed architecture docs
â”œâ”€â”€ SEMANTIC_MEMORY.md         # Memory system documentation
â””â”€â”€ README.md                  # This file
```

---

## ğŸ§  Semantic Memory System

### How It Works

1. **Storage**: Every accepted conversation is embedded and stored in Qdrant
   - Uses OpenAI `text-embedding-3-small` (1536 dimensions)
   - Includes query, response, mode, and metadata
   - Quality-filtered by critic agent (only ACCEPT decisions)

2. **Retrieval**: Intent agent retrieves relevant memories before processing
   - Top-k semantic search with cosine similarity
   - Configurable threshold (default: 0.75)
   - Formatted context injected into agent prompts

3. **Benefits**:
   - **Personalization**: Remembers user preferences
   - **Context**: Understands conversation history
   - **Learning**: Improves responses over time
   - **Continuity**: Works across sessions

### Memory Types

- `conversation`: General Q&A interactions
- `preference`: User preferences (e.g., "I prefer bar charts")
- `fact`: User-declared facts (e.g., "I'm working on X project")
- `pattern`: Common query patterns
- `error_resolution`: Problem-solving history

### Example

**First interaction:**
```
User: "Show me sales data as a bar chart"
Assistant: [Generates bar chart]
ğŸ’¾ Stores: User prefers bar charts for sales data
```

**Later interaction:**
```
User: "Show me revenue trends"
Assistant: [Retrieves memory about chart preference]
           [Automatically generates bar chart]
```

See [SEMANTIC_MEMORY.md](docs/SEMANTIC_MEMORY.md) for detailed documentation.

---

## ğŸ“Š Data Analyst Mode

### Features

- **Natural language to SQL**: Generate queries from plain English
- **Automated visualizations**: Smart chart type selection
- **Interactive charts**: Plotly-based visualizations
- **Result interpretation**: Natural language summaries

### Supported Chart Types

- **Bar Chart**: Comparisons, rankings
- **Line Chart**: Trends over time
- **Pie Chart**: Proportions, distributions
- **Scatter Plot**: Correlations, relationships

### Example Queries

```
"Show me sales by region"
â†’ SQL: SELECT region, SUM(sales) FROM sales GROUP BY region
â†’ Chart: Bar chart with regions on x-axis

"How have prices changed over time?"
â†’ SQL: SELECT date, AVG(price) FROM products GROUP BY date
â†’ Chart: Line chart showing price trends

"What's the distribution of car types?"
â†’ SQL: SELECT type, COUNT(*) FROM cars GROUP BY type
â†’ Chart: Pie chart showing proportions
```

---

## ğŸ” Advanced RAG Features

### Chunking Strategies

- **Semantic chunking**: Split by meaning, not just length
- **Overlap**: Maintains context between chunks
- **Metadata preservation**: Source, page numbers, timestamps

### Retrieval Options

- **Hybrid search**: Combines semantic + keyword search
- **Metadata filtering**: Filter by source, date, type
- **Reranking**: Re-scores results for relevance
- **Source attribution**: Shows where answers come from

### Document Support

- **PDF**: Automatic text extraction
- **TXT**: Plain text files
- **Markdown**: Preserves formatting
- **Batch ingestion**: Process entire directories

---

## ğŸ›ï¸ Configuration

### Key Settings (config/settings.py)

```python
# Model Configuration
OPENAI_MODEL = "gpt-4o-mini"
EMBEDDING_MODEL = "text-embedding-3-small"
EMBEDDING_DIMENSIONS = 1536

# Retrieval Settings
TOP_K_RETRIEVAL = 3
MIN_SIMILARITY_SCORE = 0.7

# Memory Settings
MEMORY_TOP_K = 3
MEMORY_MIN_SCORE = 0.75

# Chunking
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 200
```

### Environment Variables

See `.env.example` for all available configuration options.

---

## ğŸ› Troubleshooting

### Common Issues

**1. Qdrant Connection Error**
```
Error: Cannot connect to Qdrant
```
**Solution**: Ensure Qdrant is running on `localhost:6333`
```bash
docker ps | grep qdrant  # Check if running
```

**2. OpenAI API Error**
```
Error: Invalid API key
```
**Solution**: Check `.env` file has correct `OPENAI_API_KEY`

**3. No Memories Stored**
```
Memory count remains at 3
```
**Solution**: 
- Check critic is accepting answers (look for âœ… in terminal)
- Ensure Qdrant collection exists
- Verify semantic memory is enabled

**4. Import Errors**
```
ModuleNotFoundError: No module named 'X'
```
**Solution**: Reinstall dependencies
```bash
pip install -r requirements.txt
```

### Debug Mode

Enable detailed logging:
```python
# In config/settings.py
DEBUG_MODE = True
```

Look for these debug indicators in terminal:
- ğŸ’¾ Memory Write Node
- âœ… Stored semantic memory
- â­ï¸ Skipping memory storage
- ğŸ“¦ Retrieval node
- ğŸ” Query analysis

---

## ğŸ“ˆ Observability

### Langfuse Integration

Lumiere integrates with Langfuse for comprehensive observability:

1. **Traces**: Full request lifecycle tracking
2. **Token usage**: Cost monitoring per operation
3. **Latency**: Performance metrics
4. **Agent behavior**: Decision tracking

**Setup:**
1. Create account at [langfuse.com](https://langfuse.com)
2. Add keys to `.env`
3. View traces in Langfuse dashboard

### Memory Statistics

View memory stats in terminal:
```bash
python -c "from memory.semantic_memory import get_memory_stats; import json; print(json.dumps(get_memory_stats(), indent=2))"
```

Example output:
```json
{
  "total_memories": 15,
  "vector_size": 1536,
  "memory_types": {
    "conversation": 10,
    "preference": 3,
    "fact": 1,
    "pattern": 1
  }
}
```

---

## ğŸ¤ Contributing

We welcome contributions! Please see our contributing guidelines.

### Development Setup

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

### Code Style

- Follow PEP 8
- Use type hints
- Add docstrings to functions
- Keep functions focused and small

---

## ğŸ“ Documentation

Full documentation is available in the [`docs/`](docs/) folder:

- **[Quick Start Guide](docs/QUICKSTART.md)**: Get up and running in 5 minutes
- **[Architecture Guide](docs/GRAPH_ARCHITECTURE.md)**: Detailed workflow documentation
- **[Semantic Memory Guide](docs/SEMANTIC_MEMORY.md)**: Memory system documentation
- **[Contributing Guide](docs/CONTRIBUTING.md)**: How to contribute
- **[Changelog](docs/CHANGELOG.md)**: Version history and updates
- **[Documentation Index](docs/DOCUMENTATION.md)**: Complete documentation overview

## ğŸ§ª Testing

Comprehensive test suite with 34 tests covering core functionality:

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=. --cov-report=html

# Run specific test file
pytest tests/test_semantic_memory.py
```

**Test Coverage:**
- âœ… Semantic Memory (9 tests)
- âœ… Intent Agent (6 tests)
- âœ… Graph Workflow (10 tests)
- âœ… RAG Components (10 tests)

See [`tests/README.md`](tests/README.md) for complete testing guide and [`TEST_SETUP_SUMMARY.md`](TEST_SETUP_SUMMARY.md) for current status.

---

## ğŸ—ºï¸ Roadmap

### Current Features âœ…
- Multi-agent RAG system
- Semantic memory integration
- SQL data analysis
- Automated visualizations
- Critic-based quality control
- Langfuse observability

### Coming Soon ğŸš§
- [ ] Multi-user support with user isolation
- [ ] Memory pruning and consolidation
- [ ] Advanced query routing
- [ ] Custom embedding models
- [ ] API endpoints (REST/GraphQL)
- [ ] Memory analytics dashboard
- [ ] Feedback loop for memory refinement

### Future Vision ğŸ”®
- [ ] Multi-modal support (images, audio)
- [ ] Agent collaboration framework
- [ ] Distributed memory architecture
- [ ] Real-time streaming responses
- [ ] Plugin system for extensibility

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

Built with:
- [LangChain](https://www.langchain.com/) - LLM framework
- [LangGraph](https://langchain-ai.github.io/langgraph/) - Agent orchestration
- [Qdrant](https://qdrant.tech/) - Vector database
- [Streamlit](https://streamlit.io/) - UI framework
- [OpenAI](https://openai.com/) - LLM & embeddings
- [Langfuse](https://langfuse.com/) - Observability

---

## ğŸ“§ Contact

For questions, issues, or feedback:
- Open an issue on GitHub
- Check existing documentation
- Review troubleshooting section

---

## â­ Star History

If you find Lumiere useful, please consider giving it a star! â­

---

**Made with â¤ï¸ for the AI community**